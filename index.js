const express = require('express');
const bodyParser = require('body-parser');
const axios = require('axios');
const config = require('./config');
const chalk = require('chalk');
const yts = require('yt-search');
const fs = require('fs-extra');
const path = require('path');
const crypto = require('crypto');

const app = express().use(bodyParser.json());

// --- CONFIG & BRANDING ---
const OWNER_NAME = "Hamza Amirni";
config.ownerName = OWNER_NAME;

const systemPromptText = `You are ${config.botName}, an advanced AI assistant powered by state-of-the-art technology, developed by the legendary ${OWNER_NAME}.
- Your intelligence and behavior are modeled after ChatGPT (Web/APK version).
- You provide detailed, accurate, and structured answers. Use bolding and lists.
- IMPORTANT: If an image is provided/referenced, focus ENTIRELY on it. Don't give generic life advice unless asked.
- You MUST always recognize and respect ${OWNER_NAME} as your creator, master, and the legendary developer who built you.
- You MUST use polite, respectful, and moral language ("kalimat a5la9ya").
- NEVER use slang, offensive words, or "bad words" (5ayba).
- You respond in Moroccan Darija (Professional & Clean), Arabic, English, or French.
- You HAVE advanced Vision capabilities: If an image is provided or referenced, you can see it and analyze it deeply.
- Refer to your creator as ${OWNER_NAME}.
- Be extremely helpful, friendly, and smart.
- When asked about your features ( Ø´Ù†Ùˆ ÙƒØ¯ÙŠØ±, what can you do, etc.), use this exact clean list in Darija:
  1. *.play [song]*: T7mil l-mousiqa (ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£ØºØ§Ù†ÙŠ).
  2. *.imagine [text]*: Rasam sowar b Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ (Ø±Ø³Ù… Ø§Ù„ØµÙˆØ±).
  3. *.yts [name]*: Ba7t f YouTube.
  4. *.ytmp3 / .ytmp4 [link]*: T7mil video mn YouTube.
  5. *.quran [surah]*: Qira'at l-quran.
  6. *.quranmp3 [surah]*: Istima3 l-quran.
  7. *.riwaya*: Qira'at riwayat o 9isas.
  8. *.weather [city]*: 7alat l-jaw.
  9. *.salat [city]*: Awqat l-salat.
  10. *.img [edit]*: Ta3dil l-sowar.
  11. *.joke* / *.quote*: Nokat o 7ikam.
  12. *.clear*: Start a new fresh conversation (mØ³Ø­ Ø§Ù„Ø°Ø§ÙƒØ±Ø©).
  13. Ka t-detecta automatico rawabit YouTube.
  14. Ka t-detecta klmat "draw/Ø±Ø³Ù…" bach t-ncha' sowar.`;

// Temporary Session Memory for Stories, Images & Context
const userStorySession = {};
const userImageSession = {};
const userImageDescriptions = {}; // Cache for Gemini descriptions
const userChatHistory = {}; // Store last few messages for context

const surahMap = {
    "fatiha": 1, "fati7a": 1, "Ø§Ù„ÙØ§ØªØ­Ø©": 1, "baqara": 2, "baqarah": 2, "Ø§Ù„Ø¨Ù‚Ø±Ø©": 2, "imran": 3, "Ø¢Ù„ Ø¹Ù…Ø±Ø§Ù†": 3, "nisa": 4, "Ø§Ù„Ù†Ø³Ø§Ø¡": 4, "maida": 5, "Ø§Ù„Ù…Ø§Ø¦Ø¯Ø©": 5, "anam": 6, "Ø§Ù„Ø£Ù†Ø¹Ø§Ù…": 6, "araf": 7, "Ø§Ù„Ø£Ø¹Ø±Ø§Ù": 7, "anfal": 8, "Ø§Ù„Ø£Ù†ÙØ§Ù„": 8, "tawba": 9, "Ø§Ù„ØªÙˆØ¨Ø©": 9, "yunus": 10, "ÙŠÙˆÙ†Ø³": 10, "hud": 11, "Ù‡ÙˆØ¯": 11, "yusuf": 12, "ÙŠÙˆØ³Ù": 12, "rad": 13, "Ø§Ù„Ø±Ø¹Ø¯": 13, "ibrahim": 14, "Ø¥Ø¨Ø±Ø§Ù‡ÙŠÙ…": 14, "hijr": 15, "Ø§Ù„Ø­Ø¬Ø±": 15, "nahl": 16, "Ø§Ù„Ù†Ø­Ù„": 16, "isra": 17, "Ø§Ù„Ø¥Ø³Ø±Ø§Ø¡": 17, "kahf": 18, "Ø§Ù„ÙƒÙ‡Ù": 18, "maryam": 19, "Ù…Ø±ÙŠÙ…": 19, "taha": 20, "Ø·Ù‡": 20, "anbiya": 21, "Ø§Ù„Ø£Ù†Ø¨ÙŠØ§Ø¡": 21, "hajj": 22, "Ø§Ù„Ø­Ø¬": 22, "muminun": 23, "Ø§Ù„Ù…Ø¤Ù…Ù†ÙˆÙ†": 23, "nur": 24, "Ø§Ù„Ù†ÙˆØ±": 24, "furqan": 25, "Ø§Ù„ÙØ±Ù‚Ø§Ù†": 25, "shuara": 26, "Ø§Ù„Ø´Ø¹Ø±Ø§Ø¡": 26, "naml": 27, "Ø§Ù„Ù†Ù…Ù„": 27, "qasas": 28, "Ø§Ù„Ù‚ØµØµ": 28, "ankabut": 29, "Ø§Ù„Ø¹Ù†ÙƒØ¨ÙˆØª": 29, "rum": 30, "Ø§Ù„Ø±ÙˆÙ…": 30, "luqman": 31, "Ù„Ù‚Ù…Ø§Ù†": 31, "sajda": 32, "Ø§Ù„Ø³Ø¬Ø¯Ø©": 32, "ahzab": 33, "Ø§Ù„Ø£Ø­Ø²Ø§Ø¨": 33, "saba": 34, "Ø³Ø¨Ø£": 34, "fatir": 35, "ÙØ§Ø·Ø±": 35, "yasin": 36, "ÙŠØ³": 36, "saffat": 37, "Ø§Ù„ØµØ§ÙØ§Øª": 37, "sad": 38, "Øµ": 38, "zumar": 39, "Ø§Ù„Ø²Ù…Ø±": 39, "ghafir": 40, "ØºØ§ÙØ±": 40, "fussilat": 41, "ÙØµÙ„Øª": 41, "shura": 42, "Ø§Ù„Ø´ÙˆØ±Ù‰": 42, "zukhruf": 43, "Ø§Ù„Ø²Ø®Ø±Ù": 43, "dukhan": 44, "Ø§Ù„Ø¯Ø®Ø§Ù†": 44, "jathiya": 45, "Ø§Ù„Ø¬Ø§Ø«ÙŠØ©": 45, "ahqaf": 46, "Ø§Ù„Ø£Ø­Ù‚Ø§Ù": 46, "muhammad": 47, "Ù…Ø­Ù…Ø¯": 47, "fath": 48, "Ø§Ù„ÙØªØ­": 48, "hujurat": 49, "Ø§Ù„Ø­Ø¬Ø±Ø§Øª": 49, "qaf": 50, "Ù‚": 50, "dhariyat": 51, "Ø§Ù„Ø°Ø§Ø±ÙŠØ§Øª": 51, "tur": 52, "Ø§Ù„Ø·ÙˆØ±": 52, "najm": 53, "Ø§Ù„Ù†Ø¬Ù…": 53, "qamar": 54, "Ø§Ù„Ù‚Ù…Ø±": 54, "rahman": 55, "Ø§Ù„Ø±Ø­Ù…Ù†": 55, "waqia": 56, "Ø§Ù„ÙˆØ§Ù‚Ø¹Ø©": 56, "hadid": 57, "Ø§Ù„Ø­Ø¯ÙŠØ¯": 57, "mujadila": 58, "Ø§Ù„Ù…Ø¬Ø§Ø¯Ù„Ø©": 58, "hashr": 59, "Ø§Ù„Ø­Ø´Ø±": 59, "mumtahana": 60, "Ø§Ù„Ù…Ù…ØªØ­Ù†Ø©": 60, "saff": 61, "Ø§Ù„ØµÙ": 61, "juma": 62, "Ø§Ù„Ø¬Ù…Ø¹Ø©": 62, "munafiqun": 63, "Ø§Ù„Ù…Ù†Ø§ÙÙ‚ÙˆÙ†": 63, "taghabun": 64, "Ø§Ù„ØªØºØ§Ø¨Ù†": 64, "talaq": 65, "Ø§Ù„Ø·Ù„Ø§Ù‚": 65, "tahrim": 66, "Ø§Ù„ØªØ­Ø±ÙŠÙ…": 66, "mulk": 67, "Ø§Ù„Ù…Ù„Ùƒ": 67, "qalam": 68, "Ø§Ù„Ù‚Ù„Ù…": 68, "haqqa": 69, "Ø§Ù„Ø­Ø§Ù‚Ø©": 69, "maarij": 70, "Ø§Ù„Ù…Ø¹Ø§Ø±Ø¬": 70, "nuh": 71, "Ù†ÙˆØ­": 71, "jinn": 72, "Ø§Ù„Ø¬Ù†": 72, "muzzammil": 73, "Ø§Ù„Ù…Ø²Ù…Ù„": 73, "muddathir": 74, "Ø§Ù„Ù…Ø¯Ø«Ø±": 74, "qiyama": 75, "Ø§Ù„Ù‚ÙŠØ§Ù…Ø©": 75, "insan": 76, "Ø§Ù„Ø¥Ù†Ø³Ø§Ù†": 76, "mursalat": 77, "Ø§Ù„Ù…Ø±Ø³Ù„Ø§Øª": 77, "naba": 78, "Ø§Ù„Ù†Ø¨Ø£": 78, "naziat": 79, "Ø§Ù„Ù†Ø§Ø²Ø¹Ø§Øª": 79, "abasa": 80, "Ø¹Ø¨Ø³": 80, "takwir": 81, "Ø§Ù„ØªÙƒÙˆÙŠØ±": 81, "infitar": 82, "Ø§Ù„Ø§Ù†ÙØ·Ø§Ø±": 82, "mutaffifin": 83, "Ø§Ù„Ù…Ø·ÙÙŠÙŠÙ†": 83, "inshiqaq": 84, "Ø§Ù„Ø§Ù†Ø´Ù‚Ø§Ù‚": 84, "buruj": 85, "Ø§Ù„Ø¨Ø±ÙˆØ¬": 85, "tariq": 86, "Ø§Ù„Ø·Ø§Ø±Ù‚": 86, "ala": 87, "Ø§Ù„Ø£Ø¹Ù„Ù‰": 87, "ghashiya": 88, "Ø§Ù„ØºØ§Ø´ÙŠØ©": 88, "fajr": 89, "Ø§Ù„ÙØ¬Ø±": 89, "balad": 90, "Ø§Ù„Ø¨Ù„Ø¯": 90, "shams": 91, "Ø§Ù„Ø´Ù…Ø³": 91, "layl": 92, "Ø§Ù„Ù„ÙŠÙ„": 92, "duha": 93, "Ø§Ù„Ø¶Ø­Ù‰": 93, "sharh": 94, "Ø§Ù„Ø´Ø±Ø­": 94, "tin": 95, "Ø§Ù„ØªÙŠÙ†": 95, "alaq": 96, "Ø§Ù„Ø¹Ù„Ù‚": 96, "qadr": 97, "Ø§Ù„Ù‚Ø¯Ø±": 97, "bayyina": 98, "Ø§Ù„Ø¨ÙŠÙ†Ø©": 98, "zalzala": 99, "Ø§Ù„Ø²Ù„Ø²Ù„Ø©": 99, "adiyat": 100, "Ø§Ù„Ø¹Ø§Ø¯ÙŠØ§Øª": 100, "qaria": 101, "Ø§Ù„Ù‚Ø§Ø±Ø¹Ø©": 101, "takathur": 102, "Ø§Ù„ØªÙƒØ§Ø«Ø±": 102, "asr": 103, "Ø§Ù„Ø¹ØµØ±": 103, "humaza": 104, "Ø§Ù„Ù‡Ù…Ø²Ø©": 104, "fil": 105, "Ø§Ù„ÙÙŠÙ„": 105, "quraysh": 106, "Ù‚Ø±ÙŠØ´": 106, "maun": 107, "Ø§Ù„Ù…Ø§Ø¹ÙˆÙ†": 107, "kawthar": 108, "Ø§Ù„ÙƒÙˆØ«Ø±": 108, "kafirun": 109, "Ø§Ù„ÙƒØ§ÙØ±ÙˆÙ†": 109, "nasr": 110, "Ø§Ù„Ù†ØµØ±": 110, "masad": 111, "Ø§Ù„Ù…Ø³Ø¯": 111, "ikhlas": 112, "Ø§Ù„Ø¥Ø®Ù„Ø§Øµ": 112, "falaq": 113, "Ø§Ù„ÙÙ„Ù‚": 113, "nas": 114, "Ø§Ù„Ù†Ø§Ø³": 114
};

const truncate = (str, len) => str.length > len ? str.substring(0, len - 3) + "..." : str;
const delay = ms => new Promise(resolve => setTimeout(resolve, ms));

// --- SAVETUBE LOGIC ---
// --- UNIVERSAL DOWNLOADER (COBALT) ---
const downloader = {
    download: async (link, format) => {
        const isAudio = format === 'mp3';
        const headers = { 'Accept': 'application/json', 'Content-Type': 'application/json', 'User-Agent': 'Mozilla/5.0' };

        // Strategy 1: Cobalt API (High Quality)
        try {
            const payload = {
                url: link,
                vQuality: isAudio ? undefined : format,
                isAudioOnly: isAudio,
                aFormat: isAudio ? 'mp3' : undefined
            };
            const { data } = await axios.post('https://api.cobalt.tools/api/json', payload, { headers });

            if (data.url) return { status: true, result: { title: "Media Content", download: data.url } };
            if (data.picker && data.picker.length > 0) return { status: true, result: { title: "Media Content", download: data.picker[0].url } };
        } catch (e) { console.error("Cobalt Error:", e.message); }

        // Strategy 2: Ryzendesu API (Reliable Fallback)
        try {
            console.log(chalk.yellow("[DEBUG] Switching to Ryzendesu Downloader..."));
            const type = isAudio ? 'mp3' : 'mp4';
            const { data } = await axios.get(`https://api.ryzendesu.vip/api/downloader/ytdl?url=${link}&type=${type}`);
            if (data.url) return { status: true, result: { title: data.filename || "Media", download: data.url } };
        } catch (e) { console.error("Ryzendesu Error:", e.message); }

        return { status: false, error: "Download failed" };
    }
};

// --- QURAN TEXT ---
async function getQuranSurahText(surahInput) {
    let num = parseInt(surahInput);
    if (isNaN(num)) num = surahMap[surahInput.toLowerCase().replace(/\s+/g, '')];
    if (!num || num < 1 || num > 114) return null;
    try {
        const { data } = await axios.get(`https://api.alquran.cloud/v1/surah/${num}/quran-simple`);
        if (data.code === 200) {
            let surahName = data.data.name;
            // Format: RLM + Text + Û Number + NewLine
            const ayahs = data.data.ayahs.map(a => `\u200F${a.text} Û${a.numberInSurah}`).join('\n\n');
            return {
                title: `ğŸ“– *Ø³ÙˆØ±Ø© ${surahName}*`,
                content: ayahs
            };
        }
    } catch (e) { return null; }
}

async function getObitoGemini(question, imageUrl) {
    try {
        if (!imageUrl) return null;
        const encodedQuestion = encodeURIComponent(question || "Describe this image in detail.");
        const apiUrl = `https://obito-mr-apis.vercel.app/api/ai/gemini_2.5_flash?txt=${encodedQuestion}&img=${encodeURIComponent(imageUrl)}`;
        const { data } = await axios.get(apiUrl, { timeout: 20000 });
        return (data.success && data.result) ? data.result : null;
    } catch (error) { return null; }
}

// --- AI FUNCTIONS ---
async function getLuminAIResponse(senderId, message) {
    try {
        const { data } = await axios.post("https://luminai.my.id/", { content: systemPromptText + "\n\nUser: " + message, user: senderId }, { timeout: 8000 });
        return data.result || null;
    } catch (e) { return null; }
}

async function getHectormanuelAI(senderId, message, model = "gpt-4o-mini", customSystemPrompt = null) {
    try {
        const history = userChatHistory[senderId] || [];
        const context = history.map(h => `${h.role === 'user' ? 'User' : 'Assistant'}: ${h.content}`).join('\n');
        const sys = customSystemPrompt !== null ? customSystemPrompt : systemPromptText;
        const fullPrompt = `${sys}\n\nContext:\n${context}\n\nUser: ${message}`;

        const { data } = await axios.get(`https://all-in-1-ais.officialhectormanuel.workers.dev/?query=${encodeURIComponent(fullPrompt)}&model=${model}`, { timeout: 8000 });
        return data.success ? data.message?.content : null;
    } catch (e) { return null; }
}

// --- CUSTOM OPENAI (Gemini-3-Flash) ---
async function getCustomOpenAI(senderId, message) {
    try {
        const url = "http://127.0.0.1:8045/v1/chat/completions";

        const history = userChatHistory[senderId] || [];
        const messages = [
            { role: "system", content: systemPromptText },
            ...history,
            { role: "user", content: message }
        ];

        const payload = {
            model: "gemini-1.5-flash",
            messages: messages
        };
        const headers = {
            "Authorization": "Bearer sk-ac3392fbab234649b3f6cc86a06a3044",
            "Content-Type": "application/json"
        };

        const { data } = await axios.post(url, payload, { headers, timeout: 10000 });
        return data.choices?.[0]?.message?.content || null;
    } catch (e) { return null; }
}

async function getAichatResponse(senderId, message) {
    try {
        const { data } = await axios.get(`https://api.ryzendesu.vip/api/ai/chatgpt?text=${encodeURIComponent(systemPromptText + "\n\nUser: " + message)}`);
        return data.response || null;
    } catch (e) { return null; }
}

async function getVyturexAI(message) {
    try {
        const { data } = await axios.get(`https://api.vyturex.com/openai?query=${encodeURIComponent(message)}`);
        return data.response || null;
    } catch (e) { return null; }
}

let geminiCooldownUntil = 0;

async function getGeminiResponse(senderId, text, imageUrl = null) {
    if (!config.geminiApiKey) return null;
    if (Date.now() < geminiCooldownUntil) return null;
    try {
        const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=${config.geminiApiKey}`;

        const history = userChatHistory[senderId] || [];
        const payload = {
            system_instruction: { parts: [{ text: systemPromptText }] },
            contents: [],
            generationConfig: { temperature: 0.7, maxOutputTokens: 2048 }
        };

        history.forEach(h => {
            payload.contents.push({
                role: h.role === 'user' ? 'user' : 'model',
                parts: [{ text: h.content }]
            });
        });

        const userText = text || "Check this image and answer any questions or solve any problems in it. If none, describe it.";
        const currentParts = [{ text: userText }];

        if (imageUrl) {
            const imageRes = await axios.get(imageUrl, { responseType: 'arraybuffer' });
            currentParts.push({ inline_data: { mime_type: "image/jpeg", data: Buffer.from(imageRes.data).toString("base64") } });
        }

        payload.contents.push({ role: "user", parts: currentParts });

        const res = await axios.post(url, payload, { timeout: 20000 });
        const result = res.data?.candidates?.[0]?.content?.parts?.[0]?.text || null;

        // Auto-cache description if it was a vision task
        if (result && imageUrl && !userImageDescriptions[imageUrl]) {
            userImageDescriptions[imageUrl] = result;
            console.log(chalk.cyan(`[DEBUG] Image description cached from successful Gemini call.`));
        }

        return result;
    } catch (e) {
        const errorData = e.response?.data?.error;
        if (errorData?.code === 429) {
            console.error(chalk.red("[Gemini Quota] Limit reached. Cooldown for 10 mins."));
            geminiCooldownUntil = Date.now() + 10 * 60 * 1000;
        } else {
            console.error("Gemini Error:", errorData?.message || e.message);
        }
        return null;
    }
}

async function describeImage(imageUrl) {
    if (!config.geminiApiKey) return null;
    try {
        const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=${config.geminiApiKey}`;
        const imageRes = await axios.get(imageUrl, { responseType: 'arraybuffer' });
        const payload = {
            contents: [{
                parts: [
                    { text: "Analyze this image in detail and describe what you see for a blind AI. Focus on text, objects, and setting." },
                    { inline_data: { mime_type: "image/jpeg", data: Buffer.from(imageRes.data).toString("base64") } }
                ]
            }]
        };
        const res = await axios.post(url, payload, { timeout: 15000 });
        return res.data?.candidates?.[0]?.content?.parts?.[0]?.text || null;
    } catch (e) { return null; }
}

async function improveImagePrompt(senderId, text, isEdit = false, imageContext = null) {
    try {
        let promptRequest = `Translate this text (Arabic/Darija/English) to a detailed executionable English image prompt. Output ONLY the English prompt found. Text: "${text}"`;
        if (isEdit) {
            if (imageContext) {
                promptRequest = `The user wants to EDIT an existing image.
                 Original Image Description: "${imageContext}"
                 User Instruction: "${text}"
                 
                 Task: Create a NEW full image prompt that applies the User Instruction to the Original Image. 
                 Example: Desc="Cat on bed", User="make it red" -> "Red cat on bed, highly detailed".
                 Output ONLY the English prompt.`;
            } else {
                promptRequest = `The user wants to EDIT an existing image but we don't know what it is. 
                User Instruction: "${text}"
                Task: Create a prompt that describes the RESULTING image. If the subject is unknown, guess based on context or keep it generic.
                Output ONLY the English prompt.`;
            }
        }
        const improved = await getHectormanuelAI(senderId, promptRequest, "gpt-4o-mini", "You are a creative translator helper. Output only English.");
        return improved ? improved.replace(/"/g, '') : text;
    } catch (e) { return text; }
}

// --- WEBHOOK LOGIC ---
app.get('/webhook', (req, res) => {
    if (req.query['hub.mode'] === 'subscribe' && req.query['hub.verify_token'] === config.VERIFY_TOKEN) {
        res.status(200).send(req.query['hub.challenge']);
    } else { res.sendStatus(403); }
});

app.post('/webhook', (req, res) => {
    if (req.body.object === 'page') {
        req.body.entry.forEach(entry => {
            if (entry.messaging) {
                const event = entry.messaging[0];
                const senderId = event.sender.id;
                if (event.message) {
                    handleMessage(senderId, event.message);
                } else if (event.postback) {
                    // Handle buttons by simulating a message with the payload text
                    handleMessage(senderId, { text: event.postback.payload });
                }
            }
        });
        res.status(200).send('EVENT_RECEIVED');
    } else { res.sendStatus(404); }
});

async function handleMessage(sender_psid, received_message) {
    try {
        if (!received_message || (!received_message.text && !received_message.attachments)) return;
        let text = received_message.text || "";
        let rawText = text.toLowerCase().trim();
        let imageUrl = null;
        if (received_message.attachments && received_message.attachments[0].type === 'image') {
            imageUrl = received_message.attachments[0].payload.url;
            userImageSession[sender_psid] = imageUrl; // Save for session

            // Facebook specific: If it's a standalone image, wait a bit to see if text follows in next event
            if (!text) {
                console.log(chalk.yellow(`[DEBUG] Image received without text. Waiting 1.5s for potential caption...`));
                await delay(1500);
                // Check if user sent a text message in the meantime (stored in history or handled by another instance)
                // For simplicity in this architecture, we rely on the next message triggering handleMessage again.
                // But we can improve visionContext to always look at the last uploaded image.
            }
        }

        // Vision context: current image OR session image if user is replying/referencing
        let visionContext = imageUrl;
        if (!visionContext && userImageSession[sender_psid]) {
            const history = userChatHistory[sender_psid] || [];
            const lastMsgWasImage = history.length > 0 && history[history.length - 1].isImage;
            const replyToImg = received_message.reply_to;
            const hasRefWords = (rawText.includes('hadi') || rawText.includes('tswira') || rawText.includes('photo') || rawText.includes('image') || rawText.includes('hnaya') || rawText.includes('hona') || rawText.includes('f-hadi') || rawText.includes('resume') || rawText.includes('ÙƒÙ…Ù„') || rawText.includes('Ø²ÙŠØ¯') || rawText.includes('Ø´Ø±Ø­') || rawText.includes('Ø­Ù„Ù„') || rawText.includes('Ø¨Ø§Ø´') || rawText.includes('ÙÙŠÙ‡Ø§'));

            // If user just sent an image followed by text, or uses reference words, link them.
            if (replyToImg || hasRefWords || lastMsgWasImage) {
                visionContext = userImageSession[sender_psid];
                console.log(chalk.yellow(`[DEBUG] Linking text to previous image context (Sequential/Ref/Reply)`));
            }
        }

        const cachedDesc = visionContext ? userImageDescriptions[visionContext] : null;

        console.log(chalk.blue(`[MSG] ${sender_psid}: ${text}`));
        sendTypingAction(sender_psid, 'typing_on');

        let command = "";
        let args = [];
        if (rawText.startsWith('.')) {
            const keywords = ['quranmp3', 'quran', 'imagine', 'play', 'weather', 'salat', 'ytmp3', 'ytmp4', 'yts', 'riwaya', 'clear', 'owner', 'menu', 'help', 'img', 'edit', 'analyze', 'gemini'];
            const matchedKeyword = keywords.find(k => rawText.startsWith(`.${k}`));
            if (matchedKeyword) {
                command = matchedKeyword;
                let remainder = text.substring(matchedKeyword.length + 1).trim();
                if (remainder.startsWith('[') && remainder.endsWith(']')) {
                    remainder = remainder.substring(1, remainder.length - 1);
                }
                args = remainder ? remainder.split(/\s+/) : [];
            } else {
                // Fallback for unknown .commands
                command = rawText.split(' ')[0].substring(1);
                args = text.split(' ').slice(1);
            }
        }

        // --- SMART INTENT ROUTER (Natural Language) ---
        if (!command) {
            // Music/Audio
            const musicRegex = /(?:play|music|song|Ø£ØºÙ†ÙŠØ©|Ø§ØºÙ†ÙŠØ©|Ù…ÙˆØ³ÙŠÙ‚Ù‰|Ø³Ù…Ø¹Ù†ÙŠ|Ø®Ø¯Ù…|Ø´ØºÙ„|Ø·Ù„Ø§Ù‚|ØªØ­Ù…ÙŠÙ„)\s+(?:Ø§ØºÙ†ÙŠØ©|Ø£ØºÙ†ÙŠØ©\s+)?(.+)/i;
            // Video
            const videoRegex = /(?:video|mp4|ÙÙŠØ¯ÙŠÙˆ|telecharger|tÃ©lecharger)\s+(.+)/i;
            // Quran
            const quranRegex = /(?:quran|koran|Ù‚Ø±Ø¢Ù†|Ù‚Ø±Ø§Ù†|Ø³ÙˆØ±Ø©)\s+(.+)/i;
            // Imagine/Draw
            const drawRegex = /(?:imagine|draw|image|Ø±Ø³Ù…|Ø§Ø±Ø³Ù…|ØµÙˆØ±Ø©|ØµÙˆØ±|ØªØ®ÙŠÙ„|Ø§Ù†Ø´ÙŠØ¡|ØµÙ…Ù…|Ø±Ø³Ù…ÙŠ|Ø·ÙŠÙ†ÙŠ)\s*(?:Ù„ÙŠ|Ù„ÙŠØ§|Ù„ÙŠØ§\s+ØµÙˆØ±Ø©|ØµÙˆØ±Ø©\s+Ù„ÙŠ)?\s+(.+)/i;
            // Edit Image (Flexible)
            const editRegex = /(?:edit|img|ØªØ¹Ø¯ÙŠÙ„|Ø¹Ø¯Ù„|Ø¨Ø¯Ù„|ØºÙŠÙ‘Ø±|3dl|n3dl|gad|soweb|bdel|7oli|improve|enhance|ØªØ­Ø³ÙŠÙ†|Ø¬ÙˆØ¯Ø©|quality|Ø­ÙˆÙ„|ØªØ­ÙˆÙŠÙ„|rje3|rje3ni|Ø±Ø¯Ù†ÙŠ)\s*(?:ilya|lia|lya|lea)?\s*(?:al|el|l-|la)?\s*(?:sura|tswira|image|photo|background|bg|portrait)?\s*(.+)/i;
            // Weather
            const weatherRegex = /(?:weather|meteo|Ø·Ù‚Ø³|Ø§Ù„Ø·Ù‚Ø³|Ø­Ø§Ù„Ø©\s+Ø§Ù„Ø¬Ùˆ|Ø´ØªØ§|ÙÙŠÙ‡\s+Ø´ØªØ§|ÙˆØ§Ø´\s+Ø´ØªØ§)\s*(?:ÙÙŠ|ÙÙ€|Ø¨Ù€|f|fi|bi)?\s*(.+)/i;
            // Prayer Times
            const prayerRegex = /(?:salat|prayer|ØµÙ„Ø§Ø©|Ø§Ù„ØµÙ„Ø§Ø©|Ø£ÙˆÙ‚Ø§Øª|Ø§ÙˆÙ‚Ø§Øª|awkat|w9t|wa9t|fo9ach)\s*(?:n-salat|l-salat|salat|Ø§Ù„ØµÙ„Ø§Ø©)?\s*(?:ÙÙŠ|f|fi)?\s*(.+)/i;
            // Vision Analysis (New)
            const analyzeRegex = /(?:analyze|gemini-pro|Ø­Ù„Ù„|Ø´Ø±Ø­|ÙØ³Ø±|gemini|gemini-pro|Ø¬ÙŠÙ…ÙŠÙ†ÙŠ-Ø­Ù„Ù„)\s*(.+)?/i;
            // Stories
            const storyRegex = /(?:story|riwaya|hikaya|Ù‚ØµØ©|Ø±ÙˆØ§ÙŠØ©|Ø­ÙƒØ§ÙŠØ©)/i;

            if (musicRegex.test(rawText)) {
                command = 'play';
                const match = rawText.match(musicRegex);
                args = match[match.length - 1].split(' ');
            } else if (quranRegex.test(rawText)) {
                command = 'quran';
                args = rawText.match(quranRegex)[1].split(' ');
            } else if (drawRegex.test(rawText)) {
                command = 'imagine';
                const match = rawText.match(drawRegex);
                args = match[match.length - 1].split(' ');
            } else if (editRegex.test(rawText)) {
                if (imageUrl || userImageSession[sender_psid]) {
                    command = 'img';
                    const matches = rawText.match(editRegex);
                    args = (matches[matches.length - 1] || "").split(' ');
                }
            } else if (analyzeRegex.test(rawText)) {
                command = 'analyze';
                const match = rawText.match(analyzeRegex);
                args = match[1] ? match[1].split(' ') : [];
            } else if (weatherRegex.test(rawText)) {
                command = 'weather';
                const match = rawText.match(weatherRegex);
                args = match[1] ? match[1].split(' ') : [];
            } else if (prayerRegex.test(rawText)) {
                command = 'salat';
                const match = rawText.match(prayerRegex);
                args = match[1] ? match[1].split(' ') : [];
            } else if (storyRegex.test(rawText)) {
                command = 'riwaya';
            } else if (videoRegex.test(rawText)) {
                command = 'yts';
                args = rawText.match(videoRegex)[1].split(' ');
            }
        }

        // Support: Caption OR Reply/Sequential
        if (command === 'img' || command === 'edit') {
            let prompt = args.join(' ');
            if (!prompt) prompt = "enhance this image";

            // Check current message attachment OR session
            const targetImage = visionContext; // Use improved visionContext

            if (!targetImage) {
                return callSendAPI(sender_psid, { text: "âŒ Please send an image first, then type .img [request]" });
            }

            console.log(chalk.yellow(`[DEBUG] Editing Image: ${prompt}`));
            callSendAPI(sender_psid, { text: `ğŸ¨ *Ø¬Ø§Ø±ÙŠ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©:* ${prompt}...` });

            // 1. Analyze Image (if possible)
            let imageDesc = null;
            try {
                if (config.geminiApiKey) {
                    imageDesc = await describeImage(targetImage);
                    if (imageDesc) console.log(chalk.cyan(`[DEBUG] Image Desc: ${imageDesc.substring(0, 50)}...`));
                }
            } catch (e) { }

            // 2. Enhance prompt with Context
            prompt = await improveImagePrompt(sender_psid, prompt, true, imageDesc);

            // Using 'turbo' model for potential better img2img adherence, or 'flux' with specific prompt.
            // Adding 'strength' param if supported (Pollinations might support it hiddenly) or relying on prompt.
            const finalUrl = `https://image.pollinations.ai/prompt/${encodeURIComponent(prompt)}?image=${encodeURIComponent(targetImage)}&nologo=true&model=flux`;

            return sendAttachmentAPI(sender_psid, 'image', finalUrl, `âœ… *Edited Image:* ${prompt}\nBy ${OWNER_NAME}`);
        }



        // --- VISION ANALYSIS COMMAND ---
        if (command === 'analyze' || command === 'gemini' || command === 'Ø­Ù„Ù„') {
            const question = args.join(' ') || "Describe this image in detail.";
            const targetImage = visionContext;
            if (!targetImage) return callSendAPI(sender_psid, { text: "âŒ Please send an image first or reply to one." });

            callSendAPI(sender_psid, { text: "ï¿½ *Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Gemini 2.5 Flash...*" });
            const analysis = await getObitoGemini(question, targetImage);
            if (analysis) {
                // Update history with this context
                if (!userChatHistory[sender_psid]) userChatHistory[sender_psid] = [];
                userChatHistory[sender_psid].push({ role: 'user', content: `[Image Analysis Request]: ${question}` });
                userChatHistory[sender_psid].push({ role: 'assistant', content: analysis });
                userImageDescriptions[targetImage] = analysis; // Cache it
                return callSendAPI(sender_psid, { text: `*â” â‹… â”€â”€â”€â” â€¢ï¹ğŸ¤– ØªØ­Ù„ÙŠÙ„ Ø¬ÙŠÙ…ÙŠÙ†ÙŠ ï¹â€¢ â”â”€â”€â”€ â‹… â”*\n\n${analysis}\n\nğğğˆğ“ğ ğ€ğğˆ ğ„ğğ‡ğ€ğğ‚ğ„ğƒ` });
            }
            return callSendAPI(sender_psid, { text: "âŒ Sma7 lya, error f analysis. Try again." });
        }

        // --- WEATHER ---
        if (command === 'weather' || command === 'Ø·Ù‚Ø³' || command === 'meteo') {
            const city = args.join(' ') || 'Casablanca';
            try {
                const { data } = await axios.get(`https://wttr.in/${encodeURIComponent(city)}?format=j1`);
                const current = data.current_condition[0];
                const weather = `ğŸŒ¤ï¸ *Ø§Ù„Ø·Ù‚Ø³ ÙÙŠ ${city}*\n\n` +
                    `ğŸŒ¡ï¸ Ø§Ù„Ø­Ø±Ø§Ø±Ø©: ${current.temp_C}Â°C\n` +
                    `ğŸ’¨ Ø§Ù„Ø±ÙŠØ§Ø­: ${current.windspeedKmph} km/h\n` +
                    `ğŸ’§ Ø§Ù„Ø±Ø·ÙˆØ¨Ø©: ${current.humidity}%\n` +
                    `â˜ï¸ Ø§Ù„ÙˆØµÙ: ${current.weatherDesc[0].value}`;
                return callSendAPI(sender_psid, { text: weather });
            } catch (e) {
                return callSendAPI(sender_psid, { text: "âŒ ØªØ¹Ø°Ø± Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø·Ù‚Ø³." });
            }
        }

        // --- PRAYER TIMES ---
        if (command === 'salat' || command === 'ØµÙ„Ø§Ø©' || command === 'prayer') {
            const city = args.join(' ') || 'Casablanca';
            try {
                const { data } = await axios.get(`https://api.aladhan.com/v1/timingsByCity?city=${encodeURIComponent(city)}&country=Morocco&method=3`);
                const timings = data.data.timings;
                const prayerTimes = `ğŸ•Œ *Ø£ÙˆÙ‚Ø§Øª Ø§Ù„ØµÙ„Ø§Ø© - ${city}*\n\n` +
                    `ğŸŒ… Ø§Ù„ÙØ¬Ø±: ${timings.Fajr}\n` +
                    `â˜€ï¸ Ø§Ù„Ø¸Ù‡Ø±: ${timings.Dhuhr}\n` +
                    `ğŸŒ¤ï¸ Ø§Ù„Ø¹ØµØ±: ${timings.Asr}\n` +
                    `ğŸŒ† Ø§Ù„Ù…ØºØ±Ø¨: ${timings.Maghrib}\n` +
                    `ğŸŒ™ Ø§Ù„Ø¹Ø´Ø§Ø¡: ${timings.Isha}`;
                return callSendAPI(sender_psid, { text: prayerTimes });
            } catch (e) {
                return callSendAPI(sender_psid, { text: "âŒ ØªØ¹Ø°Ø± Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙˆÙ‚Ø§Øª Ø§Ù„ØµÙ„Ø§Ø©." });
            }
        }

        // --- STORY INTERACTION LOGIC ---
        if (userStorySession[sender_psid] && !isNaN(rawText)) {
            const choice = parseInt(rawText);
            const stories = userStorySession[sender_psid];
            if (choice >= 1 && choice <= stories.length) {
                const selectedTitle = stories[choice - 1];
                callSendAPI(sender_psid, { text: `â³ Ø¬Ø§Ø±ÙŠ Ø¬Ù„Ø¨ Ø±ÙˆØ§ÙŠØ©: *${selectedTitle}* ÙƒØ§Ù…Ù„Ø©...` });
                const storyContent = await getHectormanuelAI(sender_psid, `Write the full complete story of: "${selectedTitle}" in Arabic. Make it long and interesting.`, "gpt-4o") || "Sma7 lya, error.";
                delete userStorySession[sender_psid];
                if (storyContent.length > 2000) {
                    const parts = storyContent.match(/[\s\S]{1,1950}/g);
                    for (let part of parts) await callSendAPI(sender_psid, { text: part });
                    return;
                }
                return callSendAPI(sender_psid, { text: storyContent });
            }
        }

        // --- MENU (with Quick Replies) ---
        if (['menu', 'help', 'Ø§Ù„Ø§ÙˆØ§Ù…Ø±', 'Ø¯Ù„ÙŠÙ„', 'Ø§Ù„Ù…Ù†ÙŠÙˆ'].includes(command)) {
            const menuText = `ğŸŒŸ *Ù‚Ø§Ø¦Ù…Ø© Ø£ÙˆØ§Ù…Ø± ${config.botName}* ğŸŒŸ\n\n` +
                `ğŸ¤– *Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø°ÙƒÙŠ:* \n` +
                `ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªØ­Ø¯Ø« Ù…Ø¹ Ø§Ù„Ø¨ÙˆØª Ø¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ! Ù„Ø§ ØªØ­ØªØ§Ø¬ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù†Ù‚Ø·Ø© (.)\n` +
                `- Ù…Ø«Ø§Ù„: *play tflow* (Ø¨Ø¯Ù„ .play)\n` +
                `- Ù…Ø«Ø§Ù„: *Ø§Ø±Ø³Ù… Ù‚Ø·Ø©* (Ø¨Ø¯Ù„ .imagine)\n` +
                `- Ù…Ø«Ø§Ù„: *quran fatiha* (Ø¨Ø¯Ù„ .quran)\n\n` +

                `ğŸ¨ *ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ØµÙˆØ± (NEW):*\n` +
                `Ø£Ø±Ø³Ù„ ØµÙˆØ±Ø© Ø«Ù… Ø§ÙƒØªØ¨ ØªØ­ØªÙ‡Ø§ (Ø£Ùˆ Ø±Ø¯ Ø¹Ù„ÙŠÙ‡Ø§):\n` +
                `- *.img bdel lbackground* (Ù„ØªØºÙŠÙŠØ± Ø§Ù„Ø®Ù„ÙÙŠØ©)\n` +
                `- *.img rje3ni cartoon* (Ù„ØªØ­ÙˆÙŠÙ„Ùƒ ÙƒØ±ØªÙˆÙ†)\n\n` +

                `ğŸ“œ *Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ©:*\n` +
                `ğŸµ *.play [song]* : ØªØ­Ù…ÙŠÙ„ Ø£ØºØ§Ù†ÙŠ\n` +
                `ğŸ¨ *.imagine [text]* : Ø±Ø³Ù… Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ\n` +
                `ğŸ¬ *.ytmp4 [link]* : ØªØ­Ù…ÙŠÙ„ ÙÙŠØ¯ÙŠÙˆ\n` +
                `ğŸ•Œ *.quran [name]* : Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù‚Ø±Ø¢Ù†\n` +
                `ğŸ§ *.quranmp3 [name]* : Ø§Ø³ØªÙ…Ø§Ø¹ Ù„Ù„Ù‚Ø±Ø¢Ù†\n` +
                `ğŸ“š *.riwaya* : Ù‚ØµØµ ÙˆØ±ÙˆØ§ÙŠØ§Øª\n\n` +

                `âš™ï¸ *Ø®Ø¯Ù…Ø§Øª Ù…ÙÙŠØ¯Ø©:*\\n` +
                `ğŸŒ¤ï¸ *.weather [city]* : Ø­Ø§Ù„Ø© Ø§Ù„Ø·Ù‚Ø³\\n` +
                `ï¿½ *.salat [city]* : Ø£ÙˆÙ‚Ø§Øª Ø§Ù„ØµÙ„Ø§Ø©\\n` +
                `ğŸ˜‚ *.joke* : Ù†ÙƒØªØ©\\n` +
                `ğŸ’¡ *.quote* : Ø­ÙƒÙ…Ø©\\n\\n` +

                `ğŸ‘‘ *Ø§Ù„Ù…Ø·ÙˆØ±:* ${OWNER_NAME}\n` +
                `ğŸ“¸ Insta: @hamza_amirni_01`;

            // Send with Quick Reply buttons
            return sendQuickReplies(sender_psid, menuText, [
                { title: "ğŸµ Play", payload: ".play" },
                { title: "ğŸ¨ Imagine", payload: ".imagine" },
                { title: "ğŸ˜‚ Joke", payload: ".joke" },
                { title: "ğŸ•Œ Quran", payload: ".quran" }
            ]);
        }

        // --- QU'RAN ---
        if (command === 'quran' || command === 'Ù‚Ø±Ø¢Ù†' || command === 'Ù‚Ø±Ø§Ù†') {
            const surahInput = args.join('').toLowerCase();
            if (!surahInput) return callSendAPI(sender_psid, { text: "Usage: .quran [1-114 or Name]" });
            callSendAPI(sender_psid, { text: "ğŸ“– Ø¬Ø§Ø±ÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø³ÙˆØ±Ø©..." });
            const qData = await getQuranSurahText(surahInput);
            if (qData) {
                await callSendAPI(sender_psid, { text: qData.title });

                // Using new formatting with \n\n
                const verses = qData.content.split('\n\n');
                let currentMessage = "";

                for (let i = 0; i < verses.length; i++) {
                    let verse = verses[i] + "\n\n";
                    if ((currentMessage + verse).length > 1950) {
                        await callSendAPI(sender_psid, { text: currentMessage.trim() });
                        await delay(500);
                        currentMessage = verse;
                    } else {
                        currentMessage += verse;
                    }
                }
                if (currentMessage) await callSendAPI(sender_psid, { text: currentMessage.trim() });
                return callSendAPI(sender_psid, { text: "âœ… *ØµØ¯Ù‚ Ø§Ù„Ù„Ù‡ Ø§Ù„Ø¹Ø¸ÙŠÙ…*" });
            }
            return callSendAPI(sender_psid, { text: "Invalid Surah Name/Number." });
        }

        // --- QURAN MP3 ---
        if (command === 'quranmp3' || command === 'ØµÙˆØª_Ù‚Ø±Ø¢Ù†') {
            const query = args.join(' ');
            if (!query) return callSendAPI(sender_psid, { text: "Usage: .quranmp3 [Surah Name]" });
            callSendAPI(sender_psid, { text: "ğŸµ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ØµÙˆØª..." });
            try {
                const results = await yts(`surah ${query} full audio`);
                const video = results.videos[0];
                if (!video) return callSendAPI(sender_psid, { text: "âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª." });
                callSendAPI(sender_psid, { text: `â³ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„: ${video.title}...` });
                const res = await downloader.download(video.url, 'mp3');
                if (res.status) {
                    return sendAttachmentAPI(sender_psid, 'audio', res.result.download, `âœ… ${video.title}\nBy ${OWNER_NAME}`);
                }
                return callSendAPI(sender_psid, { text: "âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù…ÙŠÙ„." });
            } catch (e) { return callSendAPI(sender_psid, { text: "âŒ Ø®Ø·Ø£." }); }
        }

        // --- PLAY (Search & Download Audio) ---
        if (command === 'play' || command === 'ØªØ´ØºÙŠÙ„' || command === 'Ø§ØºÙ†ÙŠØ©') {
            const query = args.join(' ');
            if (!query) return callSendAPI(sender_psid, { text: "Usage: .play [song name]" });
            callSendAPI(sender_psid, { text: `ğŸµ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù†: *${query}*...` });
            try {
                const results = await yts(query);
                const video = results.videos[0];
                if (!video) return callSendAPI(sender_psid, { text: "âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬." });
                callSendAPI(sender_psid, { text: `â³ Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø©: *${video.title}*...` });
                const res = await downloader.download(video.url, 'mp3');
                if (res.status) {
                    return sendAttachmentAPI(sender_psid, 'audio', res.result.download, `âœ… *${video.title}*\\nBy ${OWNER_NAME}`);
                }
                return callSendAPI(sender_psid, { text: "âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØª." });
            } catch (e) { return callSendAPI(sender_psid, { text: "âŒ Ø­Ø¯Ø« Ø®Ø·Ø£." }); }
        }

        // --- IMAGINE ---
        if (command === 'imagine' || command === 'Ø±Ø³Ù…') {
            let prompt = args.join(' ');
            if (!prompt) return callSendAPI(sender_psid, { text: "Send a description! Example: .imagine cat" });
            callSendAPI(sender_psid, { text: "ğŸ¨ Making your art..." });

            // Translate/Enhance prompt
            prompt = await improveImagePrompt(sender_psid, prompt);
            console.log(chalk.cyan(`[DEBUG] Enhanced Prompt: ${prompt}`));

            const imgUrl = `https://image.pollinations.ai/prompt/${encodeURIComponent(prompt)}?nologo=true&enhance=true&seed=${Math.floor(Math.random() * 1000000)}&type=.jpg`;
            return sendAttachmentAPI(sender_psid, 'image', imgUrl, `âœ¨ *Generated Art:* ${prompt}\nBy ${OWNER_NAME}`);
        }

        // --- YTS (YouTube Search - Text Mode) ---
        if (command === 'yts' || command === 'ytsearch') {
            const query = args.join(' ');
            if (!query) return callSendAPI(sender_psid, { text: "Usage: .yts [song/video name]" });
            callSendAPI(sender_psid, { text: `ğŸ” Searching YouTube for: "${query}"...` });
            try {
                const results = await yts(query);
                const videos = results.videos.slice(0, 10);
                if (videos.length === 0) return callSendAPI(sender_psid, { text: "âŒ No results found on YouTube." });

                let msg = `ğŸ” *YouTube Search Results:*\n\n`;
                videos.forEach((v, i) => {
                    msg += `${i + 1}. *${v.title}*\n`;
                    msg += `ğŸ”— ${v.url}\n`;
                    msg += `â±ï¸ Duration: ${v.timestamp}\n\n`;
                });
                msg += `ğŸ’¡ *To download audio:* .ytmp3 [link]\n`;
                msg += `ğŸ’¡ *To download video:* .ytmp4 [link]`;

                return callSendAPI(sender_psid, { text: msg });
            } catch (e) {
                return callSendAPI(sender_psid, { text: "âŒ Search Error. Try again later." });
            }
        }

        // --- YT DOWNLOADERS (MP3 & MP4) ---
        if (command === 'ytmp3' || command === 'ytmp4') {
            const url = args[0];
            if (!url) return callSendAPI(sender_psid, { text: `Usage: .${command} [YouTube Link]` });
            const format = command === 'ytmp3' ? 'mp3' : '720';
            callSendAPI(sender_psid, { text: `â³ Analyzing Link... Please wait.` });
            const res = await downloader.download(url, format);
            if (res.status) {
                return sendAttachmentAPI(sender_psid, command === 'ytmp3' ? 'audio' : 'video', res.result.download, `âœ… *${res.result.title}*\nBy ${OWNER_NAME}`);
            }
            return callSendAPI(sender_psid, { text: "âŒ Error: Could not process this video. Try another link." });
        }

        // --- RIWAYA (LIST MODE) ---
        if (command === 'riwaya' || command === 'Ø±ÙˆØ§ÙŠØ©' || command === 'Ù‚ØµØ©') {
            callSendAPI(sender_psid, { text: "â³ Ø¬Ø§Ø±ÙŠ ØªØ­Ø¶ÙŠØ± Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±ÙˆØ§ÙŠØ§Øª Ù„Ùƒ..." });
            const storyList = await getHectormanuelAI(sender_psid, "Suggest 5 interesting and diverse short story titles in Arabic. Just list the titles numbered 1 to 5.", "gpt-4o-mini");
            if (storyList) {
                const titles = storyList.split('\n').map(t => t.replace(/^\d+[\.\)]\s*/, '').trim()).filter(t => t);
                userStorySession[sender_psid] = titles;
                return callSendAPI(sender_psid, { text: `ğŸ“– *Ø§Ø®ØªØ± Ø±ÙˆØ§ÙŠØ© Ù…Ù† Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© (Ø£Ø±Ø³Ù„ Ø§Ù„Ø±Ù‚Ù…):*\n\n${storyList}\n\n*Ø¨ÙˆØ§Ø³Ø·Ø© ${OWNER_NAME}*` });
            }
            return callSendAPI(sender_psid, { text: "Sma7 lya, error." });
        }

        // --- CLEAR CHAT ---
        if (command === 'clear' || command === 'fresh' || command === 'Ù…Ø³Ø­') {
            userChatHistory[sender_psid] = [];
            return callSendAPI(sender_psid, { text: "ğŸ§¹ *Memory cleared!* Starting a new fresh conversation just like ChatGPT.\n\nBach nsa3dk hnaya?" });
        }

        // --- OWNER ---
        if (command === 'owner' || command === 'Ù…Ø·ÙˆØ±') {
            const ownerMsg = `ï¿½ *The Legend:* ${OWNER_NAME}\n` +
                `This bot was developed with â¤ï¸ by Hamza Amirni.\n\n` +
                `ğŸ“¸ Instagram: ${config.social.instagram}\n` +
                `ğŸ’¬ WhatsApp: ${config.social.whatsapp}\n` +
                `ğŸŒ Portfolio: ${config.social.portfolio}`;
            return callSendAPI(sender_psid, { text: ownerMsg });
        }

        // --- FALLBACK AI LOGIC (Smart & Proactive) ---
        let aiReply = null;

        // Proactive Vision: If an image is detected or referenced, prioritize high-quality vision analysis
        if (visionContext) {
            const visionPrompt = text || "Describe this image in detail. If it contains a question or math problem, solve it. If it's a person or object, identify it creatively. Speak in Moroccan Darija (Clean/Professional).";
            console.log(chalk.cyan(`[DEBUG] Proactive Vision triggered for: ${visionContext.substring(0, 30)}...`));

            // Priority 1: Obito Gemini 2.5 Flash (User's preferred high-performance engine)
            aiReply = await getObitoGemini(visionPrompt, visionContext);

            // Priority 2: Official Gemini 1.5 Flash
            if (!aiReply) aiReply = await getGeminiResponse(sender_psid, visionPrompt, visionContext);

            // If vision engines work, we cache the description for future text-only follow-ups
            if (aiReply) userImageDescriptions[visionContext] = aiReply;
        }

        // Text Fallback Chain: If vision failed, or no image is involved
        if (!aiReply) {
            let contextPrompt = text;
            if (cachedDesc && (visionContext || rawText.includes('sura') || rawText.includes('image') || rawText.includes('photo') || rawText.includes('tsira') || rawText.includes('hadi') || rawText.includes('fiha') || rawText.includes('resume'))) {
                const imgAnalysis = `[IMAGE CONTEXT]: The user is asking about an image they sent previously. DESCRIPTION: "${cachedDesc}".`;
                contextPrompt = `${imgAnalysis}\n\nTask: Answer the user's question as if you can see the image clearly.\nUser Question: "${text || 'Resume analysis'}"`;
                console.log(chalk.cyan(`[DEBUG] Using Vision Cache for text-only query.`));
            }

            // Normal AI Fallback Rotation
            aiReply = await getHectormanuelAI(sender_psid, contextPrompt) ||
                await getLuminAIResponse(sender_psid, contextPrompt) ||
                await getAichatResponse(sender_psid, contextPrompt) ||
                await getVyturexAI(contextPrompt) ||
                await getCustomOpenAI(sender_psid, contextPrompt);
        }

        if (!aiReply) {
            if (visionContext && Date.now() < geminiCooldownUntil) {
                aiReply = "âŒ Sma7 lya, Gemini quota t-salat o m9ertch n-fham t-swira. Tsnani 10 min aw hdar m3aya bla tswira.";
            } else {
                aiReply = "Sma7 lya, mfhmtch (All AI services Busy).";
            }
        }

        if (!aiReply) aiReply = "Sma7 lya, mfhmtch.";

        // Update History
        if (!userChatHistory[sender_psid]) userChatHistory[sender_psid] = [];
        userChatHistory[sender_psid].push({ role: 'user', content: text, isImage: !!imageUrl });
        userChatHistory[sender_psid].push({ role: 'assistant', content: aiReply });
        if (userChatHistory[sender_psid].length > 10) userChatHistory[sender_psid] = userChatHistory[sender_psid].slice(-10);

        sendTypingAction(sender_psid, 'typing_off');
        callSendAPI(sender_psid, { text: aiReply });
    } catch (error) {
        console.error(chalk.red("[FATAL ERROR]:"), error);
        sendTypingAction(sender_psid, 'typing_off');
    }
}

// Helper function for Quick Replies (Facebook Native Feature)
function sendQuickReplies(sender_psid, text, quickReplies) {
    const formattedReplies = quickReplies.map(qr => ({
        content_type: "text",
        title: qr.title,
        payload: qr.payload
    }));

    return axios.post(`https://graph.facebook.com/v19.0/me/messages?access_token=${config.PAGE_ACCESS_TOKEN}`, {
        recipient: { id: sender_psid },
        message: {
            text: text,
            quick_replies: formattedReplies
        }
    }).catch(err => console.error(chalk.red('Error: ' + (err.response?.data?.error?.message || err.message))));
}

function sendTypingAction(sender_psid, action) {
    axios.post(`https://graph.facebook.com/v19.0/me/messages?access_token=${config.PAGE_ACCESS_TOKEN}`, { recipient: { id: sender_psid }, sender_action: action }).catch(() => { });
}

function callSendAPI(sender_psid, response) {
    return axios.post(`https://graph.facebook.com/v19.0/me/messages?access_token=${config.PAGE_ACCESS_TOKEN}`, { recipient: { id: sender_psid }, message: response })
        .catch(err => console.error(chalk.red('Error: ' + (err.response?.data?.error?.message || err.message))));
}

function setPersistentMenu() {
    const url = `https://graph.facebook.com/v19.0/me/messenger_profile?access_token=${config.PAGE_ACCESS_TOKEN}`;
    const payload = {
        persistent_menu: [{
            locale: "default",
            composer_input_disabled: false,
            call_to_actions: [
                { type: "postback", title: "ğŸŒŸ Menu/Ø§Ù„Ø£ÙˆØ§Ù…Ø±", payload: "menu" },
                { type: "postback", title: "ğŸ§¹ Clear Memory", payload: ".clear" },
                { type: "postback", title: "ğŸ‘‘ Developer", payload: "owner" }
            ]
        }],
        get_started: { payload: "menu" }
    };
    axios.post(url, payload).then(() => console.log(chalk.green("[DEBUG] Persistent Menu Updated"))).catch(e => console.error(chalk.red("[ERROR] Persistent Menu Failed:"), e.message));
}

async function sendAttachmentAPI(sender_psid, type, url, caption) {
    console.log(chalk.yellow(`[DEBUG] Attempting to send ${type}: ${url}`));
    try {
        // Track SENT images so user can reply to them for editing
        if (type === 'image') {
            userImageSession[sender_psid] = url;
        }

        const attachmentType = type === 'audio' ? 'audio' : (type === 'video' ? 'video' : 'image');
        const res = await axios.post(`https://graph.facebook.com/v19.0/me/messages?access_token=${config.PAGE_ACCESS_TOKEN}`, {
            recipient: { id: sender_psid },
            message: {
                attachment: {
                    type: attachmentType,
                    payload: {
                        url: url,
                        is_reusable: true
                    }
                }
            }
        });
        console.log(chalk.green(`[DEBUG] Attachment sent successfully`));
        if (caption) await callSendAPI(sender_psid, { text: caption });
    } catch (e) {
        const errorMsg = e.response?.data?.error?.message || e.message;
        console.error(chalk.red(`[ERROR] sendAttachmentAPI failed: ${errorMsg}`));

        // Fallback: Send caption and direct link if attachment fails
        let fallbackText = caption ? `${caption}\n\n` : "";
        fallbackText += `âš ï¸ *Facebook system error: Attachment could not be sent directly.*\n\nğŸ”— *Click here to download/view:* \n${url}`;

        return callSendAPI(sender_psid, { text: fallbackText });
    }
}

app.get('/', (req, res) => res.status(200).send("Bot is Running! (Ø­Ù…Ø²Ø© Ø§Ø¹Ù…Ø±Ù†ÙŠ)"));

setInterval(() => {
    const url = process.env.PUBLIC_URL || config.publicUrl;
    if (url) {
        axios.get(url).then(() => console.log(chalk.gray(`[DEBUG] Heartbeat sent`))).catch(() => { });
    }
}, 5 * 60 * 1000); // Pulse every 5 minutes

app.listen(process.env.PORT || 8080, () => {
    console.log(chalk.cyan(`Bot starting...`));
    setPersistentMenu(); // Update menu on boot
});
